\documentclass[%
  a4paper,fontsize=11pt,abstract=on,%
  oneside,BCOR=19mm,% for print version adapt...
  %final % activate for final submission, removes draft status and date
]{scrreprt}
\defaulthyphenchar=127  % make all hyphens additional

% encoding (keep this and use UTF-8)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% select the thesis language
% \usepackage[ngerman]{babel}  % select this for a German thesis
\usepackage[english]{babel}  % select this for an English thesis


% file with common and simple user defs
\input{report-config.tex}

% file taking care of imports, setup etc.
\input{FrontBackmatter/preamble.tex}



\begin{document}
\pagenumbering{roman}
\include{FrontBackmatter/title.tex}


\begin{abstract}
This project report is part of the Search Query Writer (SQW) initiative by Fraunhofer INT, designed to assist researchers in generating comprehensive literature search queries, particularly in unfamiliar topic areas. A critical limitation in current literature search workflows is the absence of an objective evaluation framework for query performance, which has so far relied on subjective assessments. This study addresses this gap by introducing a curated dataset containing core publications, which are relevant publications identified through systematic literature reviews (SLRs) or bibliometric analysis works, across multiple research fields, alongside novel metrics to evaluate the query results. Recall and precision are traditionally used for literature search query evaluation, with recall being the primary metric to ensure the retrieval of relevant publications. However, literature searches often yield excessively large result sets, complicating the identification of core publications. To address this, we introduce \textit{semantic precision}, a metric that uses embedding space to identify possible true positives based on their semantic similarity to core publications. Our evaluation reveals that while SQW-generated queries outperform the baseline in recall, they often suffer in precision, particularly when query expansion introduces irrelevant keywords. Primarily, the main goal of this framework is to provide a systematic and automated approach to assess literature search queries. It lays the groundwork for improving tools like SQW, enabling more effective literature query generation across diverse research domains.

\end{abstract}



\cleardoublepage
\tableofcontents
% if your professor wants them...
 \listoffigures
% \listoftables

\cleardoublepage
\setcounter{page}{1}
\pagenumbering{arabic}


\input{content/introduction.tex}
\input{content/foundations.tex}
\input{content/approach.tex}
\input{content/evaluation.tex}
\input{content/conclusion.tex}



\appendix

\chapter{Appendix}

\begin{figure}
	\centering	
	\includegraphics{pics/citation-distribution.pdf}
	\caption[Field Citation Ratio per Topic]{The citation ratio per topic, showing the relative citation counts of core publications compared to the average citation frequency within their respective research fields. This illustrates how the prominence of each publication compares to typical citation levels in its field.}
	\label{fig:dataset-citation}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics{pics/year-distribution.pdf}
	\caption[Distribution of publication years per topic]{The distribution of publication years for core publications across various research topics, highlighting the historical range of studies considered in the bibliometric analyses for each field. Notably, for \textit{Cervical Myelopathy}, the lower bound of publication years was set to 1980 for improved readability, although the actual range goes back to 1953.}
	\label{fig:dataset-years}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics[scale=0.7]{pics/all-metrics-1.pdf}
	\caption[Evaluation: Experiment 1]{This figure shows the results of the first experiment across all the datasets. Initially, the issue with the clustering, MVEE, and Hull precision metrics becomes apparent in cases such as \textit{Cloud Migration} and \textit{Multicore Performance Prediction}, where the value is 0. This occurs due to a recall that is <3 for MVEE and Hull and recall of 0 for the clustering. On the other-hand the impact of the crafted F2 score is particularly evident in cases like \textit{Robotic Arthroplasty}, where the baseline score is very high. Conversely, for the predicted query, which retrieves more publications but maintains the same recall, the score is significantly lower.}\label{fig:all-metrics-1}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics[scale=0.7]{pics/all-metrics-2.pdf}
	\caption[Evaluation: Experiment 2]{This figure shows the results of the second experiment across all the SLR datasets. Surprisingly, we can see that using the SLR query does not achieve outstanding results, which is attributed to its reconstruction and adaptation to fit dimension's search engine. Notably, issues similar to those from the first experiment due to the recall of 0 are also apparent in this case.}\label{fig:all-metrics-2}
\end{figure}

%\section{Further Details on Something}

\printbibliography

\include{FrontBackmatter/declaration.tex}





\end{document}
