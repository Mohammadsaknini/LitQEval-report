\documentclass[%
  a4paper,fontsize=11pt,abstract=on,%
  oneside,BCOR=19mm,% for print version adapt...
  %final % activate for final submission, removes draft status and date
]{scrreprt}
\defaulthyphenchar=127  % make all hyphens additional

% encoding (keep this and use UTF-8)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% select the thesis language
% \usepackage[ngerman]{babel}  % select this for a German thesis
\usepackage[english]{babel}  % select this for an English thesis


% file with common and simple user defs
\input{report-config.tex}

% file taking care of imports, setup etc.
\input{FrontBackmatter/preamble.tex}



\begin{document}
\pagenumbering{roman}
\include{FrontBackmatter/title.tex}


\begin{abstract}
This project report is part of the Search Query Writer (SQW) initiative by Fraunhofer INT, designed to assist researchers in generating comprehensive literature search queries, particularly in unfamiliar topic areas. A critical limitation in current literature search workflows is the absence of an objective evaluation framework for query performance, which has so far relied on subjective assessments. This study addresses this gap by introducing a curated dataset containing core publications, which are relevant publications identified through systematic literature reviews (SLRs) or bibliometric analysis works, across multiple research fields, alongside novel metrics to evaluate the query results. Recall and precision are traditionally used for literature search query evaluation, with recall being the primary metric to ensure the retrieval of relevant publications. However, literature searches often yield excessively large result sets, complicating the identification of core publications. To address this, we introduce \textit{semantic precision}, a metric that uses embedding space to identify possible true positives based on their semantic similarity to core publications. Our evaluation reveals that while SQW-generated queries outperform the baseline in recall, they often suffer in precision, particularly when query expansion introduces irrelevant keywords. Primarily, the main goal of this framework is to provide a systematic and automated approach to assess literature search queries. It lays the groundwork for improving tools like SQW, enabling more effective literature query generation across diverse research domains.

\end{abstract}



\cleardoublepage
\tableofcontents
% if your professor wants them...
 \listoffigures
% \listoftables

\cleardoublepage
\setcounter{page}{1}
\pagenumbering{arabic}


\input{content/introduction.tex}
\input{content/foundations.tex}
\input{content/approach.tex}
\input{content/evaluation.tex}
\input{content/conclusion.tex}
\appendix

\chapter{Appendix}

\begin{figure}[!h]
	\centering	
	\includegraphics{pics/citation-distribution.pdf}
	\caption[Field Citation Ratio per Topic]{The citation ratio per topic, showing the relative citation counts of core publications compared to the average citation frequency within their respective research fields. This illustrates how the prominence of each publication compares to typical citation levels in its field.}
	\label{fig:dataset-citation}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics{pics/year-distribution.pdf}
	\caption[Distribution of publication years per topic]{The distribution of publication years for core publications across various research topics, highlighting the historical range of studies considered in the bibliometric analyses for each field. Notably, for \textit{Cervical Myelopathy}, the lower bound of publication years was set to 1980 for improved readability, although the actual range goes back to 1953.}
	\label{fig:dataset-years}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics[scale=0.7]{pics/all-metrics-2.pdf}
	\caption[Evaluation: Experiment 2]{This figure shows the results of the second experiment across all the SLR datasets. Surprisingly, we can see that using the SLR query does not achieve outstanding results, which is attributed to its reconstruction and adaptation to fit dimension's search engine. Notably, issues similar to those from the first experiment due to the recall of 0 are also apparent in this case. Notably, the SLR query used for \textit{Multicore Performance Prediction} is only partially available for public access \autocite{Frank2017}, hence the very low scores.}\label{fig:all-metrics-2}
\end{figure}

\begin{figure}
	\hspace*{-4cm}	
	\includegraphics[scale=0.6]{pics/baseline_results.pdf}
	\caption{The evaluation results of the baseline queries. The color in the figure represents the value of the metric in their respective columns, where the text on F2 metrics indicates the total number of relevant publications used to compute the decay factor. The query for \textit{Robotic Arthroplasty} demonstrates strong performance across precision and recall, and containing only 1.9k relevant publications, thus the high F2 score. In contrast, while \textit{Perovskite Solar Cells Stability} achieves high recall and precision, its F2 score is only decent due to the large number of publications. For the rest of the topics, the F2 score is below average mostly due to the low recall.}\label{fig:baseline-results}
	\label{fig:baseline-results}
\end{figure}

\begin{figure}
	\hspace*{-4cm}	
	\includegraphics[scale=0.6]{pics/predicted_results.pdf}
	\caption{The evaluation results of the SLR queries. The color in the figure represents the value of the metric in their respective columns, where the text on F2 metrics indicates the total number of relevant publications used to compute the decay factor. The query for \textit{Robotic Arthroplasty} demonstrates strong performance across precision and recall, and containing only 1.9k relevant publications, thus the high F2 score. In contrast, while \textit{Perovskite Solar Cells Stability} achieves high recall and precision, its F2 score is only decent due to the large number of publications. For the rest of the topics, the F2 score is below average mostly due to the low recall.}\label{fig:predicted-results}
\end{figure}

\begin{figure}
	\hspace*{-3cm}	
	\includegraphics[scale=0.6]{pics/slr_results.pdf}
	\caption{The evaluation results of the SLR queries. The color in the figure represents the value of the metric in their respective columns, where the text on F2 metrics indicates the total number of relevant publications used to compute the decay factor. Overall, the sample size of the handcrafted SLR queries is smaller compared to the baseline and predicted queries. This reduced count facilitates manual screening of results; however, it appears that precision is generally average to low across these queries. Notably, the SLR query used for \textit{Multicore Performance Prediction} is only partially available for public access \autocite{Frank2017}, hence the very low scores.} \label{fig:slr-results}

\end{figure}

\printbibliography

\include{FrontBackmatter/declaration.tex}





\end{document}
