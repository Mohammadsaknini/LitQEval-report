\documentclass[%
  a4paper,fontsize=11pt,abstract=on,%
  oneside,BCOR=19mm,% for print version adapt...
  %final % activate for final submission, removes draft status and date
]{scrreprt}
\defaulthyphenchar=127  % make all hyphens additional

% encoding (keep this and use UTF-8)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% select the thesis language
% \usepackage[ngerman]{babel}  % select this for a German thesis
\usepackage[english]{babel}  % select this for an English thesis


% file with common and simple user defs
\input{report-config.tex}

% file taking care of imports, setup etc.
\input{FrontBackmatter/preamble.tex}



\begin{document}
\pagenumbering{roman}
\include{FrontBackmatter/title.tex}


\begin{abstract}
	This project report is part of the Search Query Writer (SQW) initiative by Fraunhofer INT, designed to assist researchers in generating comprehensive literature search queries, particularly in unfamiliar topic areas. A critical limitation in current literature search workflows is the absence of an objective evaluation framework for query performance, which has so far relied on subjective assessments or the use of recall as a solitary metric. This study addresses this gap by introducing a curated dataset of core publications across multiple research fields and novel metrics to evaluate both recall and precision in query results. Recall traditionally dominates query evaluation, ensuring the retrieval of relevant publications. However, literature searches often yield excessively large result sets, complicating the identification of core documents. To address this, we introduce semantic precision, a metric that uses embedding spaces to identify true positives based on semantic similarity to core publications. Our evaluation reveals that while SQW-generated queries outperform the baseline in recall, they often suffer in precision, particularly when query expansion introduces irrelevant keywords. Nonetheless, the main goal of this  framework is to offer a systematic and automatic approach to assess literature search queries and paves the way for improving tools like SQW, enabling more effective literature query generation across diverse research domains.
\end{abstract}



\cleardoublepage
\tableofcontents
% if your professor wants them...
 \listoffigures
% \listoftables

\cleardoublepage
\setcounter{page}{1}
\pagenumbering{arabic}


\input{content/introduction.tex}
\input{content/foundations.tex}
\input{content/approach.tex}
\input{content/evaluation.tex}
\input{content/conclusion.tex}



\appendix

\chapter{Appendix}
\begin{figure}[!h]
	\vspace*{5cm}
	\centering
	\includegraphics[scale=0.35]{pics/sqw-stage1.png}
	\caption[SQW Knowledge Enrichment]{A screenshot of the SQW UI after completing the Knowledge Enrichment stage. On the left, a list of keywords is displayed alongside the number of publications associated with each keyword when used as a search term. The keywords on the right-hand side were manually categorized as general and can be roughly assessed by the number of associated publications. To narrow the scope of general keywords, we selected "agriculture" as the overarching topic. The final generated query is displayed and updated interactively as values in the transfer lists are adjusted.}

	\label{fig:sqw-stage1}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics{pics/citation-distribution.pdf}
	\caption[Field Citation Ratio per Topic]{The citation ratio per topic, showing the relative citation counts of core publications compared to the average citation frequency within their respective research fields. This illustrates how the prominence of each publication compares to typical citation levels in its field.}
	\label{fig:dataset-citation}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics{pics/year-distribution.pdf}
	\caption[Distribution of publication years per topic]{The distribution of publication years for core publications across various research topics, highlighting the historical range of studies considered in the bibliometric analyses for each field. Notably, for \textit{Cervical Myelopathy}, the lower bound of publication years was set to 1980 for improved readability, although the actual range goes back to 1953.}
	\label{fig:dataset-years}
\end{figure}


\begin{figure}[!h]
	\centering	
	\includegraphics[scale=0.7]{pics/sr-cosine-baseline.pdf}
	\caption[Semantic Cosine Similarity: Soft Robotics]{This figure illustrates the publications identified as relevant using the cosine similarity measure with the threshold $\theta$ defined by \autoref{eq:sp-cosine}, which in this case was approximately 0.547. The query results included all core publications, as expected, but also classified 75\% of the total retrieved publications as relevant.}\label{fig:sr-cosine-baseline}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics[scale=0.7]{pics/sr-hull-baseline.pdf}
	\caption[Semantic Cosine Threshold: Empirical Analysis]{This figure shows the relevant publications identified by the Convex Hull}\label{fig:sr-hull-baseline}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics[scale=0.7]{pics/all-metrics-1.pdf}
	\caption[Evaluation: Experiment 1]{This figure shows the results of the first experiment across all the datasets. Initially, the issue with the clustering, MVEE, and Hull precision metrics becomes apparent in cases such as \textit{Cloud Migration} and \textit{Multicore Performance Prediction}, where the value is 0. This occurs due to a recall that is <3 for MVEE and Hull and recall of 0 for the clustering. On the other-hand the impact of the crafted F2 score is particularly evident in cases like \textit{Robotic Arthroplasty}, where the baseline score is very high. Conversely, for the predicted query, which retrieves more publications but maintains the same recall, the score is significantly lower.}\label{fig:all-metrics-1}
\end{figure}

\begin{figure}
	\centering	
	\includegraphics[scale=0.7]{pics/all-metrics-2.pdf}
	\caption[Evaluation: Experiment 2]{This figure shows the results of the second experiment across all the SLR datasets. Surprisingly, we can see that using the SLR query does not achieve outstanding results, which is attributed to its reconstruction and adaptation to fit dimension's search engine. Notably, issues similar to those from the first experiment due to the recall of 0 are also apparent in this case.}\label{fig:all-metrics-2}
\end{figure}

%\section{Further Details on Something}

\printbibliography

\include{FrontBackmatter/declaration.tex}





\end{document}
