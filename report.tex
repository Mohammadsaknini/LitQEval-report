\documentclass[%
  a4paper,fontsize=11pt,abstract=on,%
  oneside,BCOR=19mm,% for print version adapt...
  final % activate for final submission, removes draft status and date
]{scrreprt}
\defaulthyphenchar=127  % make all hyphens additional

% encoding (keep this and use UTF-8)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% select the thesis language
% \usepackage[ngerman]{babel}  % select this for a German thesis
\usepackage[english]{babel}  % select this for an English thesis


% file with common and simple user defs
\input{report-config.tex}

% file taking care of imports, setup etc.
\input{FrontBackmatter/preamble.tex}



\begin{document}
\pagenumbering{roman}
\include{FrontBackmatter/title.tex}


\begin{abstract}
This project report is part of the Search Query Writer (SQW) initiative by Fraunhofer INT, designed to assist researchers in generating comprehensive literature search queries, particularly in unfamiliar topic areas. A critical limitation in current literature search workflows is the absence of an objective evaluation framework for query performance, which has so far relied on subjective assessments. This study addresses this gap by introducing a curated dataset containing core publications (CPs), which are relevant publications identified through systematic literature reviews (SLRs) or bibliometric analysis works, across multiple research fields, alongside novel metrics to evaluate the query results. Recall and precision are traditionally used for literature search query evaluation, with recall being the primary metric to ensure the retrieval of relevant publications. However, literature searches often yield excessively large result sets, complicating the identification of core publications. To address this, we introduce \textit{semantic precision}, a metric that uses embedding space to identify possible false negatives based on their semantic similarity to core publications. Our evaluation reveals that while SQW-generated queries outperform the baseline in recall, they often suffer in precision, particularly when query expansion introduces irrelevant keywords. Primarily, the main goal of this framework is to provide a systematic and automated approach to assess literature search queries. It lays the groundwork for improving tools like SQW, enabling more effective literature query generation across diverse research domains.

\end{abstract}



\cleardoublepage
\tableofcontents
% if your professor wants them...
 \listoffigures
% \listoftables

\cleardoublepage
\setcounter{page}{1}
\pagenumbering{arabic}


\input{content/introduction.tex}
\input{content/foundations.tex}
\input{content/approach.tex}
\input{content/evaluation.tex}
\input{content/conclusion.tex}
\appendix

\chapter{Appendix}

\begin{figure}[!h]
	\hspace*{-4cm}	
	\includegraphics[scale=0.6]{pics/baseline_results.pdf}
	\caption[Baseline Queries Results]{The evaluation results of the baseline queries. The color in the figure represents the value of the metric in their respective columns, where the additional text in brackets in the F2 metrics indicates the total number of relevant publications used to compute the decay factor. The query for \textit{Robotic Arthroplasty} demonstrates strong performance across precision and recall, and containing only 1.9k relevant publications, thus the high F2 score. In contrast, while \textit{Perovskite Solar Cells Stability} achieves high recall and precision, its F2 score is only decent due to the large number of publications. For the rest of the topics, the F2 score is below average mostly due to the low recall.}\label{fig:baseline-results}
\end{figure}

\begin{figure}
	\hspace*{-4cm}	
	\includegraphics[scale=0.6]{pics/predicted_results.pdf}
	\caption[Predicted Queries Results]{The evaluation results of the predicted queries. The color in the figure represents the value of the metric in their respective columns, where the additional text in brackets in the F2 metrics indicates the total number of relevant publications used to compute the decay factor. The query for \textit{Robotics Arthroplasty} demonstrates strong performance on recall, yet the precision is low across the board, thus the low F2 score. In contrast, while \textit{Perovskite Solar Cells Stability} achieves high recall and decent precision, its F2 score is low due to the excessive number of retrieved publications. For the rest of the topics, unlike the baseline, the F2 score is low due to either low precision or a large number of retrieved publications.}
\end{figure}

\begin{figure}
	\hspace*{-3cm}	
	\includegraphics[scale=0.6]{pics/slr_results.pdf}
	\caption[SLR Queries Results]{The evaluation results of the SLR queries. The color in the figure represents the value of the metric in their respective columns, where the additional text in brackets in the F2 metrics indicates the total number of relevant publications used to compute the decay factor. Overall, the number of retrieved publications of the handcrafted SLR queries is smaller compared to the baseline and predicted queries. This reduced count facilitates manual screening of results; however, it appears that precision is generally average to low across these queries. Notably, the SLR query used for \textit{Multicore Performance Prediction} is only partially available for public access \autocite{Frank2017}, hence the very low scores.} \label{fig:slr-results}

\end{figure}

\printbibliography

\include{FrontBackmatter/declaration.tex}





\end{document}
