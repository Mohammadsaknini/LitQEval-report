\chapter{Evaluation}\label{ch:eval}
In this section, we evaluate the performance of literature search queries based on the introduced metrics. This evaluation serves as a foundation for developing tools that can potentially generate automatic literature search queries in the future. It is crucial to note that the objective of this evaluation is not to assess the SQW tool itself, but rather to evaluate any arbitrarily generated literature search query. Thus, the focus is solely on the quality of the query, independent of the method by which it was generated.

\section{Experimental Setup}

The curated dataset is constructed using two distinct methods to identify core publications: Bibliometric Analysis (14 topics) and Systematic Literature Review (7 topics), as illustrated in \autoref{fig:dataset-overview}. For the SLRs, the original queries used by the researchers are available. Consequently, we conduct two main experiments. In both experiments we use Dimensions.ai to retrieve all required data. The retrieval process relies on their default relevance-based sorting method, which ranks publications based on the number of keyword matches between the title-abstract and the provided query.

The first experiment involves all 21 topics from both the SLRs and BAs, where we compare a baseline query against a query generated by the SQW. The baseline query consists of the exact topic name, passed into the search engine in a non-exact search fashion. For instance, the query \textit{Soft Robotics} retrieves publications containing both words in their title or abstract, even if they do not appear consecutively.

The predicted query, however, is semi-automatically generated using the SQW tool. This process begins by providing the baseline query as input, which generates a list of keywords. These keywords are then manually sorted by the author into specific or general categories, as described in \autoref{fig:sqw-stage1}. The overarching topic is derived from the topic itself; for example, in the case of \textit{Soft Robotics}, the overarching keyword \textit{Robot} is used. In some cases, the resulting queries produced excessively large results (>100k publications). To address this, keywords were filtered to limit the results to a maximum of 50k publications, balancing evaluation cost and processing speed. Importantly, the baseline query is always included in the predicted query. This ensures that recall is at least as high for the predicted query as for the baseline, making the primary goal of the evaluation to determine whether the expanded query retrieves more core publications than the baseline without becoming overly general by retrieving irrelevant publications.

The second experiment focuses exclusively on the 7 SLR topics. It uses the exact queries and results from the first experiment but compares them to the SLR queries manually crafted by experts in the field. These expert queries are designed with well-defined research questions aimed at retrieving the most relevant publications that help tackle these exact questions.

\section{Results}
Using the data from the first experiment, we computed all the metrics, namely: Cosine Precision, Clustering Precision, MVEE Precision, Hull Precision, Recall, and the F2 score for each precision metric, as shown in Figure \ref{fig:all-metrics-1}. When examining the precision metrics, the clustering precision distinctly stands out due to its high value in certain cases, which can be directly attributed to low recall. This recall issue is also evident in some instances for the MVEE and Hull metrics, such as the baseline for \textit{Drones in Agriculture}, where they are set to 0 because fewer than three retrieved core publications are available, which is the minimum number required to define a plane.

A strong correlation is observed between cosine precision and the MVEE and Hull methods, despite relying solely on UMAP embeddings to define the enclosing shapes. This highlights the robustness of these approaches in identifying semantically relevant publications. Additionally, we have two special case topics that had 0 recall, namely \textit{Cloud Migration} and \textit{Multicore Performance Prediction}. As expected, these resulted in a 0 across the board except for the cosine similarity, since it does not require any of the retrieved core publications to exist in order to compute. Instead, it only relies on the pre-computed average embeddings of the core publications. Interestingly, the results for \textit{Cloud Migration} were not considered relevant at all, which we further experimented with and found that the first relevant publication is identified at a threshold of 0.66.


Considering the F2 score, a notable example of the impact of overly large queries without any recall improvement is \textit{Robotic Arthroplasty}. Both the baseline and predicted queries achieved a recall of 0.957, but the expanded predicted query from the SQW retrieved significantly more results overall. Specifically, the predicted query retrieved 22,892 publications, of which only 2,834 were relevant based on cosine similarity. In contrast, the baseline query retrieved 2,151 documents, with 1,904 classified as relevant. This demonstrates how an excessively large query can dilute the precision without improving recall or the number of relevant documents retrieved.

In \autoref{table:expirment-1}, we can better interpret the results of the first experiment by examining the differences between the scores of the predicted query and the baseline. Here, positive values indicate that the predicted query performs better, while negative values show that the baseline outperforms the predicted query. 

As expected, the predicted query consistently achieves similar or better recall across all topics due to the inherent nature of the SQW. However, when evaluating precision, it is evident that the broader queries generated through query expansion often degrade the performance of the model. This effect is particularly visible in the F2 scores, where the increased number of irrelevant publications impacts the balance between recall and precision.

While the SQW demonstrates advantages in terms of recall, its over-expansion often leads to excessive noise in the results. This trade-off is especially clear for topics with a significant drop in precision or F2 scores due to the broader query scope.


\begin{table}
	\caption{In this table we can see the difference in values between the predicted query from the SQW and the baseline, whereby a negative value means that the baseline is better. As anticipated we at least always achieve a similar recall, but in most cases, the SQW yields better recall. However, it severely suffers in precision. When looking at the F2 value, we can see that the tool only notably outperforms the baseline on the three topics \textit{Drones in Agriculture}, \textit{Sustainable Bio Fuel Economy}, and \textit{Multicore Performance Prediction}, whereas it shows a clear disadvantage on the topics \textit{Perovskite Solar Cells Stability}, \textit{Robotic Arthroplasty}, and \textit{Cervical Myelopathy}.}
	\tiny
	\centering
	\hspace*{-1cm}
	\begin{tabular}{p{3cm}c|cccc|cccc}
		& & \multicolumn{4}{c|}{\textbf{Precision}} & \multicolumn{4}{c}{\textbf{F2}} \\ \cline{1-10}
		\multicolumn{1}{c}{\centering \textbf{Topic}} &
		\multicolumn{1}{p{1cm}|}{\centering \textbf{Recall}} &
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{Cosine}} &
		\multicolumn{1}{p{1.2cm}}{\centering \textbf{Clustering}} & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{MVEE}} & 
		\multicolumn{1}{p{0.8cm}|}{\centering \textbf{Hull}} & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{Cosine}} & 
		\multicolumn{1}{p{1.2cm}}{\centering \textbf{Clustering}} & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{MVEE}}  & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{Hull}}  \\ \hline 
		\centering{Robotic Arthroplasty} & 0.000 &  -0.761 &  -0.528 &  -0.707 &  -0.679 &  -0.560 &  -0.490 &  -0.570 &  -0.570 \\
		\centering{Soft Robotics} & \textbf{0.111} &  -0.134 &  -0.147 &  -0.292 &  -0.152 &  -0.130 &  -0.090 &  -0.400 &  -0.300 \\
		\centering{Crop Yield Prediction} & \textbf{0.109} &  -0.280 &  -0.118 &  -0.261 &  -0.234 &  -0.210 &  -0.230 &  -0.740 &  -0.530 \\
		\centering{Synthetic Biology} & \textbf{0.310} &  -0.050 &  -0.185 & \textbf{0.637} & \textbf{0.510} & \textbf{0.030} & \textbf{0.080} &  -0.420 &  -0.330 \\
		\centering{Resilience in Business and management} & \textbf{0.185} &  -0.022 &  -0.838 & \textbf{0.150} & \textbf{0.071} & \textbf{0.060} & \textbf{0.170} & \textbf{0.330} & \textbf{0.240} \\
		\centering{Cervical Myelopathy} & \textbf{0.085} &  -0.298 &  -0.299 &  -0.061 &  -0.017 &  -0.330 &  -0.250 &  -0.700 &  -0.590 \\
		\centering{Drones in Agriculture} & \textbf{0.480} &  -0.184 & \textbf{0.298} & \textbf{0.069} & \textbf{0.028} & \textbf{0.220} & \textbf{0.160} & \textbf{0.240} & \textbf{0.120} \\
		\centering{Tourism Growth Nexus} & 0.000 &  -0.562 & \textbf{0.310} & 0.000 & 0.000 &  -0.070 &  -0.040 & 0.000 & 0.000 \\
		\centering{Sustainable Biofuel Economy} & \textbf{0.260} &  -0.122 & \textbf{0.733} & \textbf{0.513} & \textbf{0.343} & \textbf{0.190} & 0.000 & \textbf{0.150} & \textbf{0.320} \\
		\centering{Perovskite Solar Cells Stability} & \textbf{0.103} &  -0.237 &  -0.213 & \textbf{0.051} & \textbf{0.082} &  -0.460 &  -0.430 &  -0.470 &  -0.540 \\
		\centering{Nanopharmaceuticals OR Nanonutraceuticals} & \textbf{0.040} & \textbf{0.003} &  -0.376 & 0.000 & 0.000 & \textbf{0.040} & \textbf{0.010} & 0.000 & 0.000 \\
		\centering{Green Warehousing} & \textbf{0.052} &  -0.227 &  -0.411 & \textbf{0.023} & \textbf{0.004} &  -0.130 &  -0.030 &  -0.530 &  -0.200 \\
		\centering{AI on Edge Devices} & \textbf{0.250} &  -0.227 & \textbf{0.050} & \textbf{0.182} & \textbf{0.138} & \textbf{0.090} &  -0.090 &  -0.050 & \textbf{0.200} \\
		\centering{Internet of Things in Healthcare} & \textbf{0.172} &  -0.203 &  -0.090 &  -0.146 &  -0.109 &  -0.050 &  -0.160 &  -0.500 &  -0.400 \\
		\centering{Software Process Line} & \textbf{0.024} &  -0.001 & \textbf{0.239} &  -0.048 &  -0.037 & 0.000 &  -0.020 &  -0.660 &  -0.300 \\
		\centering{Data Stream Processing Latency} & \textbf{0.087} &  -0.292 &  -0.480 &  -0.237 &  -0.196 &  -0.130 &  -0.150 &  -0.510 &  -0.480 \\
		\centering{Business Process Meta Models} & \textbf{0.307} &  -0.199 &  -0.233 &  -0.228 &  -0.148 &  -0.090 &  -0.050 &  -0.360 &  -0.300 \\
		\centering{Multicore Performance Prediction} & \textbf{0.273} &  -0.239 & 0.000 & \textbf{0.158} & \textbf{0.100} & \textbf{0.200} & \textbf{0.010} & \textbf{0.400} & \textbf{0.320} \\
		\centering{Cloud Migration} & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
		\centering{Software Fault Prediction Metrics} & \textbf{0.312} &  -0.604 & \textbf{0.671} & \textbf{0.006} & \textbf{0.012} &  -0.050 & \textbf{0.020} &  -0.090 &  -0.010 \\
		\centering{Software Defect Prediction} & \textbf{0.186} &  -0.466 &  -0.311 &  -0.121 &  -0.233 &  -0.040 & \textbf{0.030} &  -0.390 &  -0.300 \\
	\end{tabular}\label{table:expirment-1}
\end{table}


The results of the second experiment, which compare the actual search queries used to identify the core publications in the SLRs, have yielded surprising yet explainable outcomes. It is important to re-emphasize that the queries initially used for the SLRs were adapted to fit dimension's query criteria and were only applied to search the title and abstract. In contrast, the original queries were utilized across a variety of search indices, such as title, abstract, full text, and sometimes full data for specific fields, which is a form query fine-tuning that is search engine specific. Therefore, it is important to note that the results will always be search engine dependent, which, in this case, is dimensions.ai.

As shown in \autoref{fig:all-metrics-2}, the results of the SLR queries are not as anticipated, particularly since they were expected to yield high recall. This discrepancy arises from the queries' reconstruction and adaptation to fit dimensions' criteria. The performance difference between the SLR queries and the baseline can be observed in \autoref{table:expirment-2}. Overall, the baseline and the SLR queries performed on nearly equal footing, with two topics favoring the SLR and three favoring the baseline. 

Interestingly, in case where the recall of the baseline significantly outperformed the SLR, namely \textit{Software Fault Prediction Metrics}, the cosine precision was drastically lower. This results in an unwanted behavior which can be interpreted by the cosine-F2 score being 0, indicating that the recall gain was of no value due to the excessive number of irrelevant retrieved publications.

\begin{table}
	\caption{This table displays the differences in values between the predicted query from the SLR and the baseline, where a negative value indicates that the baseline performs better.}
	\tiny
	\centering
	\hspace*{-1cm}
	\begin{tabular}{p{3cm}c|cccc|cccc}
		& & \multicolumn{4}{c|}{\textbf{Precision}} & \multicolumn{4}{c}{\textbf{F2}} \\ \cline{1-10}
		\multicolumn{1}{c}{\centering \textbf{Topic}} &
		\multicolumn{1}{p{1cm}|}{\centering \textbf{Recall}} &
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{Cosine}} &
		\multicolumn{1}{p{1.2cm}}{\centering \textbf{Clustering}} & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{MVEE}} & 
		\multicolumn{1}{p{0.8cm}|}{\centering \textbf{Hull}} & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{Cosine}} & 
		\multicolumn{1}{p{1.2cm}}{\centering \textbf{Clustering}} & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{MVEE}}  & 
		\multicolumn{1}{p{0.8cm}}{\centering \textbf{Hull}}  \\ \hline 
		\centering{Software Process Line} &  -0.232 & \textbf{0.286} & \textbf{0.319} & \textbf{0.162} & \textbf{0.194} & \textbf{0.250} & \textbf{0.140} & \textbf{0.290} & \textbf{0.360} \\
		\centering{Data Stream Processing Latency} &  -0.073 &  -0.008 &  -0.321 &  -0.157 &  -0.125 &  -0.070 &  -0.100 &  -0.160 &  -0.160 \\
		\centering{Business Process Meta Models} & \textbf{0.269} & \textbf{0.039} &  -0.082 & \textbf{0.421} & \textbf{0.281} & \textbf{0.190} & \textbf{0.090} & \textbf{0.390} & \textbf{0.390} \\
		\centering{Multicore Performance Prediction} & 0.000 & \textbf{0.217} & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
		\centering{Cloud Migration} & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
		\centering{Software Fault Prediction Metrics} & \textbf{0.562} &  -0.596 & \textbf{0.061} & \textbf{0.005} & 0.000 & 0.000 & \textbf{0.330} &  -0.040 &  -0.020 \\
		\centering{Software Defect Prediction} &  -0.129 &  -0.584 &  -0.446 &  -0.619 &  -0.491 &  -0.280 &  -0.210 &  -0.750 &  -0.750 \\
	\end{tabular}\label{table:expirment-2}
\end{table}

\section{Discussion}

The primary goal of this work was to determine the quality of literature search queries, emphasizing recall, which is widely regarded as an essential measure in the research community. However, precision has historically been less emphasized due to the intrinsic nature of literature search queries, which tend to favor comprehensiveness over specificity. By developing multiple metrics to evaluate the relevance of publications in a semantic space, we successfully integrated precision into the evaluation framework by introducing semantic precision.

To calculate semantic precision, we employed four metrics: semantic cosine, clustering, MVEE, and Hull precision. Each metric demonstrated distinct advantages and limitations. Initially, we hypothesized that the cosine similarity threshold should correspond to the least similar core publication. However, in certain cases, sacrificing a core publication to substantially reduce the number of irrelevant publications proved more beneficial in terms of the F2 score. Consequently, we adopted an empirical threshold estimated by maximizing the F2 score across all topics.

For Convex Hull and MVEE, we first tested their performance using the original embeddings in their high-dimensional space. However, these metrics consistently overestimated precision, often exceeding 50\%. This discrepancy is likely due to the curse of dimensionality, which complicates the construction of accurate ellipsoids or hulls in high-dimensional spaces, possibly reflecting limitations in the embedding construction process. This observation led to the decision to switch to UMAP embeddings, which offer a reduced dimensionality and improved computational feasibility. However, while UMAP embeddings show potential, the exact amount of semantic value lost compared to the original high-dimensional embeddings remains unclear.

A common issue for clustering, MVEE, and Hull methods lies in their dependency on recall. Semantic clustering requires at least two core publications, and performance improves with more core publications as the clustering process narrows the focus on relevant publications. This limitation becomes problematic when the viable core publications count is less than two. Similarly, MVEE and Hull methods necessitate at least three core publications to construct a plane capable of enclosing other potentially relevant publications. In contrast, cosine similarity only requires the pre-computed mean embedding of the core publications and a similarity threshold. This independence from recall allows cosine similarity to determine relevance even when recall is minimal, making it a more robust metric in cases of sparse core publications.





