\chapter{Introduction}\label{ch:intro}
Fraunhofer Institute for Technological Trend Analysis (INT)\footnote{\url{https://www.int.fraunhofer.de/}} is a research institute that frequently undertakes new tasks and research questions across various fields. Often, these inquiries require systematic and robust scientific responses, even when the initial knowledge in the area may be limited at the time. Given the recurring nature of this challenge, a tool that can support researchers by providing a head start and entry point into unfamiliar fields is essential.

To address this, several internal tools have been developed to analyze large volumes of scientific data from sources like Dimensions.ai\footnote{\url{https://www.dimensions.ai/}} and Web of Science\footnote{\url{https://clarivate.com/}}. The rise of large language models (LLMs) has further enhanced the appeal and accessibility of automation across numerous domains, including scientific research, spanning from idea generation and experimental iteration to paper composition\autocite{lu2024aiscientistfullyautomated}.

In the realm of search queries, the main focus has been on text-to-SQL\autocite{dong2023c3}, where an LLM is prompted via natural language to generate a precise and valid SQL query. However, to our knowledge, there has been limited and non-diverse effort dedicated to the development of text-to-literature search queries. Thus this work introduces a pipeline and curates a dataset designed to address this gap, with a particular focus on enhancing the Fraunhofer Search Query Writer tool.

\section{Motivation}
The SQW tool is currently under development by the company and has generated significant interest among researchers. However, a primary challenge we face after testing earlier versions is evaluating the quality of the generated queries. Initially, we considered gathering human feedback from users by requesting them to rate the generated query on a scale of 0 to 5. While this approach could be useful for fine-tuning the underlying model, the quantity of feedback has so far been limited and remains subjective. This is especially problematic because the tool’s purpose is to generate queries for researchers who are new to a given topic. Consequently, if the query quality is poor, the researcher may not immediately recognize this.

Identifying suitable evaluation metrics and datasets to assess the quality of the generated queries is a complex task, which forms the basis of this master's project. The project’s objective is to find a robust solution for assessing the quality of literature search queries, enabling the further development of the SQW tool to provide more accurate results and improve productivity through the integration of large language models.


\section{Research Questions}\label{sec:researchQuestions}

There are three main research questions that we aim to address while curating the dataset and formulating the metrics to evaluate the generated queries. These questions are based on the following hypothesis: Given that we know which publications are the most important for a given field, which we will refer to as \textbf{Core Publications} (CP).

\begin{itemize}
	\item \textbf{RQ1:} How many of the core publications can the generated search query recall?
	\item \textbf{RQ2:} How many of the non-core publications are relevant?
	\item \textbf{RQ3:} Which metric can we use to penalize the model for exploiting the ability to generate large queries to achieve high recall?
\end{itemize}



\section{Structure of this Work}\label{sec:structure}
The remainder of this work is structured as follows:

After this introduction, we will first focus on the foundations in \autoref{ch:foundations}, where the SQW tool will be briefly explained, primarily focusing on the format of the input and the stages that the SQW consists of. Subsequently, we will explore related works in \autoref{sec:relwork} and review the currently available datasets, explaining why they are not suitable for our specific use case.

Next, we introduce our framework, which consists of two main components: the curated dataset \autoref{sec:dataset} and the evaluation metrics \autoref{sec:eval-metrics}. In the dataset section, we explain how the data was collected and perform a dataset analysis to gain deeper insights into its characteristics. In the evaluation metrics section, we present the metrics developed to assess the performance of literature search queries.

Following this, we present an evaluation of the framework and showcase the results in \autoref{ch:eval}. This chapter begins with a description of the conducted experiments, where two types of queries are used: those written for systematic literature reviews (SLR) and those generated by the Search Query Writer, which are then used for evaluation.

Finally, we conclude this work with a summary of the main contributions and provide an outlook on future directions in \autoref{ch:conclusion}.

