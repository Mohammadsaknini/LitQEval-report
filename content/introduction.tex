\chapter{Introduction}\label{ch:intro}

The Fraunhofer Institute for Technological Trend Analysis (INT)\footnote{\url{https://www.int.fraunhofer.de/}} specializes in conducting technology foresight, tackling tasks and research questions across a diverse array of fields. These challenges often necessitate systematic and scientifically sound approaches, even when prior knowledge in the domain is sparse. To address this recurring need, a tool that assists researchers by generating effective search queries as entry points into unfamiliar subject areas becomes essential. For instance, when faced with a specific technological research question, the process typically begins with a thorough literature search using databases like Dimensions \autocite{Hook2018}, Web of Science\footnote{\url{https://clarivate.com/}}, and Scopus\footnote{\url{https://www.elsevier.com/}}. This step involves crafting a precise search query to locate relevant studies, enabling researchers to deliver foresight grounded in scientific evidence.

To address this, several internal tools such as Topic Modeling and Grants Analytics have been developed to analyze large volumes of scientific data from sources like Dimensions.ai and Web of Science. The rise of Large Language Models (LLMs) has further enhanced the appeal and accessibility of automation across numerous domains, including scientific research, spanning from idea generation and experimental iteration to paper composition \autocite{lu2024aiscientistfullyautomated}.

In the realm of search queries, the main focus has been on text-to-SQL \autocite{dong2023c3}, where an LLM is prompted via natural language to generate a precise and valid SQL query. However, to our knowledge, there has been limited effort dedicated to the development of text-to-literature search queries. Thus this work introduces an evaluation pipeline and curates a dataset designed to help address this gap, with a particular focus on enhancing the evaluation the quality of literature search queries using a novel approach called \textit{Semantic Precision}.

The evaluation of literature search queries is inherently complex due to several factors. One major challenge is the tendency to retrieve an overwhelming number of publications. In the end, only a small subset is considered relevant. Another challenge stems from the different objectives of the queries constructed. For example, Systematic Literature Reviews (SLRs) aim to identify every potentially relevant publication through exhaustive search strategies. In contrast, Bibliometric Analyses (BAs) focus on defining a large, relevant set of publications to be quantitatively evaluated. A common problem in both approaches is the initial identification of relevant publications within a large retrieved dataset.

To address this issue, we introduce Semantic Precision: a method for assessing the relevance of publications based on their semantic similarity of the title and abstract to a defined set of core publications. This approach is the basis for the construction of an adjusted $F_\beta$ metric, which includes the recall, the semantic precision, and an additional decay factor. The decay factor allows researchers to tailor the evaluation according to the specific intent of the literature review, whether it aligns with the comprehensive goals of SLRs or the quantitative focus of BAs. By accounting for these elements, our method provides a highly refined and focused framework for evaluating the effectiveness of search queries.

\section{Motivation}
The SQW tool is currently under development by Fraunhofer INT and has generated interest among researchers internally. However, a primary challenge researchers face after testing earlier versions is evaluating the quality of the generated queries. Initially, we considered gathering human feedback from users by requesting them to rate the generated query on a scale of 0 to 5. While this approach could be useful for fine-tuning the underlying model, the quantity of feedback has so far been limited and remains subjective. This is especially problematic because the tool’s purpose is to generate queries for researchers who are new to a given topic. Consequently, if the query quality is poor, the researcher may not immediately recognize this.

Identifying suitable evaluation metrics and datasets to assess the quality of the generated queries is a complex task, which forms the basis of this master's project. The project’s objective is to find a robust solution for assessing the quality of literature search queries, enabling the further development of the SQW tool to provide more accurate results and improve productivity through the integration of LLMs.


\section{Research Question}\label{sec:researchQuestions}

Our work is driven by a central research question that guides both the curation of the dataset and the formulation of metrics for evaluating the effectiveness of the generated queries. The root of this question is the following hypothesis: Given that we know important publications in a given field, referred to as Core Publications (CPs), we can design metrics to evaluate the performance of search queries based on their ability to balance relevance and specificity. This leads to the following research questions: \textbf{Which metric can effectively penalize the generation of excessively large queries that achieve high recall at the cost of precision?} By addressing this question, we aim to develop an evaluation framework that discourages the trivial exploitation of large query sizes and instead rewards meaningful query design that aligns with the intent and context of the literature search.


\section{Structure of this Work}\label{sec:structure}
The remainder of this work is structured as follows:

After this introduction, we will first focus on the foundations in \autoref{ch:foundations}, where the SQW tool will be briefly explained, primarily focusing on the format of the input and the stages that the SQW consists of. Subsequently, we will explore related works in \autoref{sec:relwork} and review the currently available datasets, explaining why they are not suitable for our specific use case.

Next, we introduce our framework, which consists of two main components: the curated dataset in \autoref{sec:dataset} and the evaluation metrics in \autoref{sec:eval-metrics}. In the dataset section, we explain how the data was collected and perform a dataset analysis to gain deeper insights into its characteristics. In the evaluation metrics section, we present the metrics developed to assess the performance of literature search queries.

Following this, we present an evaluation of the framework and showcase the results in \autoref{ch:eval}. This chapter begins with a description of the conducted experiments, where two types of queries are used: those written for systematic literature reviews (SLR) and those generated by the SQW, which are then used for evaluation.

Finally, we conclude this work with a summary of the main contributions and provide an outlook on future directions in \autoref{ch:conclusion}.

