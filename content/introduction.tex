
\chapter{Introduction}\label{ch:intro}
The Fraunhofer Institute for Technological Trend Analysis (INT)\footnote{\url{https://www.int.fraunhofer.de/}} is a research institute that frequently undertakes new tasks and research questions across various fields. Often, these inquiries require systematic and robust scientific responses, even when the initial knowledge in the area may be limited at the time. Given the recurring nature of this challenge, a tool that can support researchers by providing a head start and entry point into unfamiliar fields is essential.

To address this, several internal tools have been developed to analyze large volumes of scientific data from sources like Dimensions.ai\footnote{\url{https://www.dimensions.ai/}} and Web of Science\footnote{\url{https://clarivate.com/}}. The rise of large language models (LLMs) has further enhanced the appeal and accessibility of automation across numerous domains, including scientific research, spanning from idea generation and experimental iteration to paper composition\cite{lu2024aiscientistfullyautomated}.

In the realm of search queries, the main focus has been on text-to-SQL\cite{dong2023c3}, where an LLM is prompted via natural language to generate a precise and valid SQL query. However, to our knowledge, there has been limited and non-diverse effort dedicated to the development of text-to-literature search queries. Thus this work introduces a pipeline and curates a dataset designed to address this gap, with a particular focus on enhancing the Fraunhofer Search Query Writer tool.

\section{Motivation}
The SQW tool is currently under development by the company and has generated significant interest among researchers. However, a primary challenge we face after testing earlier versions is evaluating the quality of the generated queries. Initially, we considered gathering human feedback from users by requesting them to rate the generated query on a scale of 0 to 5. While this approach could be useful for fine-tuning the underlying model, the quantity of feedback has so far been limited and remains subjective. This is especially problematic because the tool’s purpose is to generate queries for researchers who are new to a given topic. Consequently, if the query quality is poor, the researcher may not immediately recognize this.

Identifying suitable evaluation metrics and datasets to assess the quality of the generated queries is a complex task, which forms the basis of this master's project. The project’s objective is to find a robust solution for assessing the quality of literature search queries, enabling the further development of the SQW tool to provide more accurate results and improve productivity through the integration of large language models.


\section{Research Questions}\label{sec:researchQuestions}

There are three main research questions that we aim to address while curating the dataset and formulating the metrics to evaluate the generated queries. These questions are based on the following hypothesis: Given that we know which publications are the most important for a given field, which we will refer to as \textbf{Core Publications} (CP).

\begin{itemize}
	\item \textbf{RQ1:} How many of the core publications can the generated search query recall?
	\item \textbf{RQ2:} How many of the non-core publications are relevant?
	\item \textbf{RQ3:} Which metric can we use to penalize the model for exploiting the ability to generate large queries to achieve high recall?
\end{itemize}



\section{Structure of this Work}\label{sec:structure}
The remainder of this work is structured as follows:

After this introduction, we will first focus on the foundations in \autoref{ch:foundations}, where we will briefly explain the SQW tool, primarily focusing on the format of the input and output. We will also cover the necessary basics and information about the main data source, Dimensions.ai, which will be used to curate the dataset \autoref{sec:basics}, as well as common metrics \autoref{sec:metrics} used for evaluation in the community, before discussing related works in \autoref{sec:relwork}.

Next, in \autoref{ch:ownApproach}, we will detail our approach and its components, which involve curating a dataset that contains core publications and establishing a pipeline to streamline the evaluation process, thereby accelerating the development of the SQW tool.

We will evaluate our approach in \autoref{ch:eval}, beginning with a description of our experimental setup, where we acquire the generated query via the SQW tool and explain the reasoning behind the selection of topics for which the queries are generated. 

Finally, we will conclude this work with a summary of our main contributions and an outlook in \autoref{ch:conclusion}.

