\chapter{Foundations}\label{ch:foundations}

In this section, we will begin by briefly introducing the SQW tool to provide a foundational understanding of how its settings may influence the overall results. This introduction will also establish the groundwork for designing an evaluation process that ensures a fair and accurate assessment.

Next, we will review prior works that tackle the challenge of using large language models (LLMs) to generate literature search queries, examining their potential in this domain.

Following this, we will describe the dataset curation process, including relevant statistics and exploratory data analysis. This analysis will help us evaluate the dataset's quality and determine the topics suitable for evaluation.

Then, we will introduce new metrics designed to assess the quality of generated search queries. These metrics are intended not only to identify true positives from core publications but also to account for other potentially relevant publications. Once the evaluation pipeline is established, we will conduct a comparative analysis between a baseline and the queries generated by the SQW tool.

Finally, we will provide an outlook on the next steps and potential optimization options for the SQW tool. Additionally, we will discuss other tools that could be developed to build upon these advancements.


\section{Search Query Writer}\label{sec:sqw}
The Search Query Writer is a tool based on a large language model (LLM), specifically using GPT-4o, to systematically generate literature search queries. The only required input for this tool, which is the main focus of this work, is the \textbf{Topic}. Users are required to provide a topic for generating a search query, irrespective of the scientific field—for example, \textit{Synthetic Biology}. 

Several optional inputs are available to enhance the quality of the generated query, including:
\begin{itemize}
	
\item \textbf{Negative Keywords:} Terms that should be excluded to avoid unwanted results.
\item \textbf{Description:} A description that serves as an alignment mechanism to clarify the task’s intent.
\item \textbf{Modes:} Three selectable modes (Strict, Moderate, Creative) that control the temperature of the LLM to manage the level of randomness in responses.
\item \textbf{Depth:} A parameter that specifies how comprehensively the topic should be analyzed.
\item \textbf{Supporting Documents:} Users can upload a PDF, ideally a survey or overview document on the topic, which helps the tool acquire knowledge about the scientific field and better align with the research intent.

\end{itemize}
These additional inputs are intended to refine and tailor the search query to more closely match the user's research goals, but will not be extensively tested in this work.

To generate a literature search query, we designed the SQW to take a human-like approach, divided into two main steps: \textbf{Knowledge Enrichment} and \textbf{Iterative Scientific Fine-Tuning}.

The objective of the Knowledge Enrichment step is to provide the LLM with contextual information about the research topic. This is achieved by first retrieving information from Wikipedia based on the given topic. Specifically, the first 4,000 characters from the top-\( k \) pages are collected and summarized before being passed into the LLM's memory. ArXiv is queried in a similar manner to gather relevant research content. Additionally, we perform an online search using DuckDuckGo\footnote{\url{https://duckduckgo.com/}}, aggregating results to offer a broader understanding of the topic.

\begin{figure}
	\centering
	\includegraphics{FrontBackmatter/H-BRS_Logo_A4}
	\caption[Nice short caption]{Nice long caption}
	\label{fig:sqw-overview}
\end{figure}

Notably, each of the steps is conducted within a separate memory session, with results stored independently for future use. This setup allows the model to explore the topic using various sources, helping to mitigate any potential recency bias and ensure a well-rounded context.

The output of this first stage will be a list of keywords that are usually presented in a transfer-list as shown in \autoref{fig:sqw-stage1}, that contains two lists; specific and general keywords, along side some additional information such as the number of publications found per keyword. The goal is to let the user decide weather a keyword is too broad in which he is supposed to move it to the general list, and if it targets the specified topic quite well then it should stay in the specific list, and at the end they user should also provide an overarching topic for which the scope of the general keywords is limited to be more focused to words the research intent. The output of this step will be the queries that will be used for the evaluation at the end.

The iterative scientific fine-tuning on the other hand approaches more scientific sources, namely dimensions.ai, which is a literature database that offers quick access to publications across a wide range of journals. The query generated in the earlier stage is then used to prompt dimensions three times, once to retrieve the most cited 1k literature, a second time to retrieve the newest 1k literature, and one last 1k to retrieve the most relevant documents based on their altmetric rating. This leaves us with a total of 3k publications in which we extract the title and abstract for, and use an Openai's embedding model, and apply a simple RAG pipeline to retrieve the most relevant keywords based on the extracted passages.


\begin{figure}
	\centering
	\includegraphics[scale=0.3]{pics/sqw-stage1.png}
	\caption[Nice short caption]{Nice long caption}
	\label{fig:sqw-stage1}
\end{figure}

\section{Related Work}\label{sec:relwork}

Systematic literature reviews are widely used across various fields, allowing researchers to conduct a comprehensive manual review of scientific topics and identify publications that answer a set of important research questions. However, one significant challenge with this approach has been the exponential growth in the number of publications, which makes conducting unbiased reviews increasingly difficult. In the age of technological advancements, we can now leverage these technologies to assist in investigating topics without the need to manually sift through extensive lists of publications. To address this issue, a series of works have been proposed within the Conference and Labs of the Evaluation Forum (CLEF) \autocite{kanoulas2017clef, kanoulas2018clef, kanoulas2019clef}. These works focus on the evaluation of empirical medical research, utilizing a dataset of medical literature. They introduce two primary tasks: Task 1, which involves identifying relevant studies from the PubMed medical database, and Task 2, which assesses the ranking of studies following title and abstract screening. Notably, the evaluation pipeline, along with the dataset and descriptions of these tasks, are publicly accessible on GitHub\footnote{\url{https://github.com/CLEF-TAR/tar/tree/master}}.

Large Language Models (LLMs) have had a significant impact on modern technology, including in scientific research, where they have provided remarkable improvements in speed. While the processing speed of LLMs is unprecedented, the quality of their output in various domains is still being explored. The work by Wang \autocite{wang2023can} investigated the performance of ChatGPT in generating Boolean search queries for literature reviews. Specifically, it evaluated the effectiveness of ChatGPT in constructing queries for systematic literature reviews using different prompting techniques, including zero-shot, few-shot and iterative guided approaches. The evaluation used the CLEF datasets \autocite{kanoulas2017clef, kanoulas2018clef, kanoulas2019clef} and an additional medical dataset containing a collection of seeds \autocite{Wang_2022}. Although the results highlight the limitations of ChatGPT's performance, this work underscores the potential of LLMs to aid literature review, especially when supported by examples or more advanced, structured pipelines.

A broader and more diverse evaluation of the quality of automatically generated literature search queries for systematic literature reviews was conducted by Badami \autocite{badami2023adaptive}. In this work, they introduced a pipeline that generates literature search queries based on a given research question and abstracts from previously identified relevant publications. The evaluation was performed against a dataset they constructed, which contains the results of 10 systematic literature reviews, including candidate papers, queries used, and relevant papers identified in each review. For example, in the review $SLR_1$, a total of 7,002 candidate papers were retrieved using search query $S$, from which a subset of 59 relevant papers $RP$ was identified. To assess their proposed approach, they compared the generated queries in various settings using recall and precision metrics, benchmarking them against the original search query $S$. The dataset is made public ally available on Zenodo\footnote{\url{https://tinyurl.com/496zuar3}}.

